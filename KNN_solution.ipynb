{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors\n",
    "Author: Carleton Smith; W.P.G. Peterson\n",
    "\n",
    "Revised: Jessica Cervi\n",
    "\n",
    "Expected time = 3 hours\n",
    "\n",
    "Total points = 95 points\n",
    "\n",
    "Assignment Overview\n",
    "K-Nearest Neighnours (KNN) is a reasonably simple algorithm that is easy to grasp and can be very effective. This assignment will test your abilities in three different parts. In the first part, you will familiarize yourself with the problem and data. In the second part, you will code a KNN Classifier from scratch, evaluate performance, and compare to Scikit-Learn's implementation. Finally, in the last part you will interpret results and explain findings.\n",
    "\n",
    "In the first part, we will revisit Bayes' formula and evaluate your ability to calculate simple Bayesian posterior probabilities. In the second part, we will ask you to build functions that calculate the parameters of Bayesian posteriors for Bayesian Linear Regression.\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort coding in Python while also helping you review key topics from each module. As you progress through the assignment, answers will get increasingly complex. It is important that you adopt a data scientist's mindset when completing this assignment. Remember to run your code from each cell before submitting your assignment. Running your code beforehand will notify you of errors and give you a chance to fix your errors before submitting it. You should view your Vocareum submission as if you are delivering a final project to your manager or client.\n",
    "\n",
    "Vocareum Tips\n",
    "\n",
    "Do not add arguments or options to functions unless asked specifically. This will cause an error in Vocareum.\n",
    "Do not use a library unless asked explicitly in the question.\n",
    "You can download the Grading Report after submitting the assignment. It will include the feedback and hints on the incorrect questions.\n",
    "Learning Objectives\n",
    "Have a firm understanding of KNN algorithm\n",
    "Practice running through the data science workflow to solve a problem\n",
    "Demonstrate how to translate a mathematical algoritm into effective code\n",
    "Understand common pitfalls when working with distances\n",
    "Determine class balance in classification problems\n",
    "Use Euclidian-distance to find the distance between vectors\n",
    "Code KNN from scratch\n",
    "Implement KNN using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Nearest Neighbors\n",
    "Importing the data set and exploratory data analysis\n",
    "For this assignment, we will be using a database from the UC Irvine Machine Learning Repositiory that can be downloaded from the following link: Human Activity Recognition Using Smartphones Data Set.\n",
    "\n",
    "Please see the Data Folder to explore the data files further.\n",
    "\n",
    "In this assignment, we will try to classify the type of activity a person is performing based on measurements collected from a smartphone. The activities include:\n",
    "\n",
    "Walking\n",
    "Walking_Upstairs\n",
    "Walking_Downstairs\n",
    "Sitting\n",
    "Standing\n",
    "Laying\n",
    "Before coding an algorithm, we will take a look at our data using Python's pandas. For visualizations, we will use the matplotlib ans seaborn libraries.\n",
    "\n",
    "Let's import the necessary libraries and load the datasets. We will be using using the pandas pd.read_table() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = './data/features.txt'\n",
    "TRAIN_DATA = './data/X_train.txt'\n",
    "TRAIN_LABELS = './data/y_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a `pandas` DataFrame objects\n",
    "\n",
    "# read feature names\n",
    "feats = pd.read_csv(FEATURE_NAMES, sep='\\n', header=None)\n",
    "\n",
    "# read in training data\n",
    "har_train = pd.read_csv(TRAIN_DATA, sep='\\s+', header=None)\n",
    "\n",
    "# read in training labels\n",
    "har_train_labels = pd.read_csv(TRAIN_LABELS, sep='\\n', header=None, names=[\"label\"], squeeze = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "har_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 1:\n",
    "* 5 points\n",
    "\n",
    "Find out how many rows and columns are in har_train. Assign your answer to the variable ans1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans1 = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "ans1 = har_train.shape\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the first 5 rows of the DataFrame of feature names feats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assign each feature name in feats to each column of har_train using the .columns attribute and we visuaize the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "har_train.columns = feats.iloc[:, 0].values\n",
    "har_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "10 points\n",
    "\n",
    "Use the iloc() function to extract the first 20 features of the dataframe har_train. Save this new dataframe to first_twenty.\n",
    "\n",
    "Next, using the seaborn library create a heatmap for the correlation matrix.\n",
    "\n",
    "First you have to create the correlation matrix from the pandas dataframe (save it in a dataframe called corr) and then plot it using seaborn with these customizations:\n",
    "\n",
    "Set the seaborn style to white.\n",
    "Generate a mask using np.triu(np.ones_like()) with the dtype as boolean to only show the lower triangle of the correlation matrix. Save it in a variable called mask.\n",
    "Set up the figure with matplotlib with figsize=(11,9). Use fig, ax = ...\n",
    "Generate a custom diverging colormap for the heatmap with the arguments (220, 10, as_cmap=True). Save it in a variable called cmap.\n",
    "Draw the heatmap with the mask and correct aspect ratio, using the arguments corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}).\n",
    "Finally, use fig.tight_layout() just before saving the plot to produce a nicely centered graph.\n",
    "You can find more information about how to create a heatmap using seaborn here.\n",
    "\n",
    "The final plot should look like this:\n",
    "\n",
    "\n",
    "\n",
    "Save your plot as a png with the name \"plot2.png\" in the folder \"results\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# extract the first 20 features\n",
    "first_twenty = har_train.iloc[:, :20] \n",
    "# compute correlation matrix\n",
    "corr = first_twenty.corr()\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate custom colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"results/plot2.png\")\n",
    "plt.close()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, exploratory data analysis (EDA) is used to develop an idea what the data we are about to work with looks like. In particular, looking for Null values and correlated features are important steps in order to:\n",
    "\n",
    "See if any features will not be useful in models because of null values.\n",
    "See if any model assumptions are violated by correlated features (such as in linear / logistic regression).\n",
    "For the next question, we will use the dataframe containing the the target variable (har_train_labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 3:\n",
    "* 5 points\n",
    "\n",
    "How many times does the majority class appear in our data? How many times does the minority class appear in our target data? Assign these values to ans_maj and ans_min, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "ans_maj = None\n",
    "ans_min = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "ans_maj = har_train_labels.value_counts().max()\n",
    "ans_min = har_train_labels.value_counts().min()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an observation, a large imbalance in the distribution of the target variable categories can cause machine learning algorithms to perform differently. In this case, the algorithm can perform well with the majority class and poorly on the minority class.\n",
    "\n",
    "As a final note, EDA should be performed at the beginning of each project and should be tailored to your specific problem to develop and understanding of the data for a particular purpose. Sometimes this could be a time consuming process when the data are large with many features.\n",
    "\n",
    "Note that the above example shows just a few of the actions you can perform for EDA\n",
    "\n",
    "Having performed EDA, we define a \"test\" data-set that will help us evaluate different models.\n",
    "\n",
    "The attribute train_test_split from the sklearn.model_selection module provides an easy way to do this.\n",
    "\n",
    "For this exercise we set test_size=.3 and random_state=24 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# give to targets and observations conventional names\n",
    "y = har_train_labels \n",
    "X = har_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code K-Nearest Neighbours (KNN)\n",
    "Note: The following example was adapted from example 2.1.2 in Chapter 2 of Machine Learning in Action by Peter Harrington.\n",
    "\n",
    "Before fitting a model using KNN and the built-in modules of the sklearn package, we will code our own version of KNN from scratch.\n",
    "\n",
    "As you know, KNN calculates the distance between the value (or a vector) that we want to classify and all other values (or vectors) in the training data-set. Then, the \"k\" nearest neighbors are classified based on their \"label\" and the majority is given to the predicted value.\n",
    "\n",
    "Thus, our final goal will be to define a function costum_knn that accepts the following parameters:\n",
    "\n",
    "A single data point to be classified (input_vector)\n",
    "Training data (X_train)\n",
    "Labels for training data (y_train)\n",
    "Value of k (some positive integer)\n",
    "Function definition:\n",
    "def costum_knn(input_vector, X_train, y_train, k)\n",
    "\n",
    "Pseudo Code:\n",
    "\n",
    "for every point in our dataset:\n",
    "    calculate the distance between the current point and input_vector\n",
    "    sort the distances in increasing order\n",
    "    take k items with lowest distances from input_vector\n",
    "    find the majority class among these items\n",
    "    return the majority class label from the k closest neighbors\n",
    "Return:\n",
    "\n",
    "The prediction for input_vector\n",
    "We will begin by defining some auxiliary functions that we will need to build our KNN algorithm from scratch.\n",
    "\n",
    "\n",
    "\n",
    "* Question 4:\n",
    "* 5 points\n",
    "\n",
    "Find the Euclidean distance between the points p1 = (1,2,3,-4,6) and p2 = (10,2,32,-2,0), defined below. Assign the distance as a float to ans4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "p1 = (1,2,3,-4,6)\n",
    "p2 = (10,2,32,-2,0)\n",
    "\n",
    "ans4 = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "ans4 = 0\n",
    "for i, j in zip(p1,p2):\n",
    "    ans4 += (i-j)**2\n",
    "ans4 = ans4**.5\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 5:\n",
    "* 5 points\n",
    "\n",
    "Define a function called euclid_dist that takes an input of two points represented as tuples in the format p1 = (a1, b1,...n1) and p2 = (a2, b2, ...n2). Your function should return the the euclidean distance between the two points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def euclid_dist(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidian Distance between two points\n",
    "    \n",
    "    Positional Arguments:\n",
    "        p1 -- A tuple of n numbers\n",
    "        p2 -- A tuple of n numbers\n",
    "    \n",
    "    Example:\n",
    "        p1 = (5,5)\n",
    "        p2 = (0,0)\n",
    "        p3 = (5,6,7,8,9,10)\n",
    "        p4 = (1,2,3,4,5,6)\n",
    "        print(euclid_dist(p1,p2)) #--> 7.0710678118654755\n",
    "        print(euclid_dist(p3,p4)) #--> 9.797958971132712\n",
    "    \"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def euclid_dist(p1, p2):\n",
    "    # Start with 0 distance\n",
    "    dist = 0\n",
    "    # For all pairs of values in two points,\n",
    "    # Find difference and square\n",
    "    for a, b in zip(p1,p2):\n",
    "        dist += (a-b)**2\n",
    "    \n",
    "    # Take Square Root\n",
    "    return dist**.5\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances with numpy\n",
    "In the KNN functions that we will define later in the assignment we will use the library numpy to calculate the distance more efficiently.\n",
    "\n",
    "This can be done via the following code : np.linalg.norm(p1-p2).\n",
    "\n",
    "Now that we can easily calculate the distances between any two points, we can start building our function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 6:\n",
    "* 10 points\n",
    "\n",
    "Define a function a function called \"all_distances\" that takes as inputs: an observation from a data set, e.g: har_train.iloc[50,:], and the full data set, e.g. har_train.\n",
    "\n",
    "Your function should create a list or numpy array with the distances between that observation point and all points in the full dataset. Your function should return a list dists with distances sorted from smallest to largest.\n",
    "\n",
    "Hints: Use np.linalg.norm() to find dists as described in the above cell. The smallest distance should be 0. Additionally, use the function np.apply_along_axis to apply this function over the columns (axis 1). You can find additional documentation about this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "\n",
    "def all_distances(test_point, data_set):\n",
    "    \"\"\"\n",
    "    Find and return a list of distances between the \"test_point\"\n",
    "    and all the points in \"data_set\", sorted from smallest to largest.\n",
    "    \n",
    "    Positional Arguments:\n",
    "        test_point -- a Pandas Series corresponding to a row in \"data_set\"\n",
    "        data_set -- a Pandas DataFrame\n",
    "    \n",
    "    Example:\n",
    "        test_point = har_train.iloc[50,:]\n",
    "        data_set = har_train\n",
    "        \n",
    "        print(all_distances(test_point, data_set)[:5])\n",
    "        #--> [0.0, 2.7970187358249854, 2.922792670143521, 2.966555149052483, 3.033982453218797]\n",
    "    \n",
    "    \"\"\"\n",
    "    return \n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "def all_distances(test_point, data_set):\n",
    "    # Take difference \n",
    "    diff = test_point - data_set\n",
    "    \n",
    "    # Find distance\n",
    "    dists = np.apply_along_axis(np.linalg.norm, 1, diff )\n",
    "    \n",
    "    # Sort\n",
    "    dists = np.sort(dists)\n",
    "    return dists\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 7:\n",
    "* 10 points\n",
    "\n",
    "Define a function a function called labels_of_smallest that takes the inputs: two different numpy arrays, the first one corresponding to a numeric column and the second one corresponding to a label column. Note that the i-th element of the numeric column corresponds to the i-th element of the label column. The third input should be a positive integer n.\n",
    "\n",
    "Your function should execute the following steps:\n",
    "\n",
    "concatenate the numeric and the label columns\n",
    "create a new dataframe with columns num and lab with the concatenation\n",
    "sort the values by the num column\n",
    "Your function should return a list (or numpy array) df of the n smallest labels corresponding to the n smallest values in the numeric array.\n",
    "\n",
    "NOTE: Make sure the order of labels corresponds to the order of values.\n",
    "\n",
    "Hint: The labels are found in har_train_labels or y. The function np.concatenate() might be useful for this or subsequent exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def labels_of_smallest(numeric, labels, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the n labels corresponding to the n smallest values in the \"numeric\"\n",
    "    numpy array.\n",
    "    \n",
    "    Positional Arguments:\n",
    "        numeric -- a numpy array of numbers\n",
    "        labels -- a numpy array of labels (string or numeric)\n",
    "            corresponding to the values in \"numeric\"\n",
    "        n -- a positive integer\n",
    "        \n",
    "    Example:\n",
    "        numeric = np.array([7,6,5,4,3,2,1])\n",
    "        labels = np.array([\"a\",\"a\",\"b\",\"b\",\"b\",\"a\",\"a\"])\n",
    "        n = 6\n",
    "        \n",
    "        print(labels_of_smallest(numeric, labels, n))\n",
    "        #--> np.array(['a', 'a', 'b', 'b', 'b', 'a'])\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "### BEGIN SOLUTION\n",
    "def labels_of_smallest(numeric, labels, n):\n",
    "    \n",
    "    # Create a df of the two arrays (to simplify sorting)\n",
    "    con = np.concatenate((numeric.reshape(-1,1), labels.reshape(-1,1)), axis = 1)\n",
    "    df = pd.DataFrame(con, columns = [\"num\",\"lab\"])\n",
    "    \n",
    "    # Sort\n",
    "    df = df.sort_values(by = 'num')\n",
    "    \n",
    "    # Return the top \"n\" values\n",
    "    return df['lab'].head(n).values\n",
    "### END SOLUTION    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 8:\n",
    "* 10 points\n",
    "\n",
    "For this question, look at the Counter function and the attribute .most_common().\n",
    "\n",
    "Define a function called label_voting that takes as input a non-empty numpy array of labels as input. Your function should return the value (as an integer) that appears most frequently in that array. In the case of of a tie, return the value in the tie that appears first in the array\n",
    "\n",
    "recast the labels as a list\n",
    "initiate the counter for labels to find the most common\n",
    "check to see if there is only one number in the list\n",
    "loop through the list labels to determine which appears most often\n",
    "return the value that appears most often in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def label_voting(labels):\n",
    "    \"\"\"\n",
    "    Given a numpy array of labels. Return the label that appears most frequently\n",
    "    If there is a tie for most frequent, return the label that appears first.\n",
    "    \n",
    "    Positional Argument:\n",
    "        labels -- a numpy array of labels\n",
    "    \n",
    "    Example:\n",
    "        lab1 = np.array([1,2,2,3,3])\n",
    "        lab2 = np.array([\"a\",\"a\",\"b\",\"b\",\"b\"])\n",
    "        \n",
    "        print(label_voting(lab1)) #--> 2\n",
    "        print(label_voting(lab2)) #--> \"b\"\n",
    "        \n",
    "    \"\"\"\n",
    "### BEGIN SOLUTION \n",
    "def label_voting(labels):\n",
    "    \n",
    "    # List methods used in this function, recast labels as list\n",
    "    labels = list(labels)\n",
    "    # instantiate counter, find most common, returns tuples\n",
    "    c = Counter(labels).most_common()\n",
    "    \n",
    "    # If only one value present, return that value\n",
    "    if len(c) == 1:\n",
    "        return c[0][0]\n",
    "    # IF first has majority, return first\n",
    "    if c[0][1] > c[1][1]:\n",
    "        return c[0][0]\n",
    "    \n",
    "    # Otherwise, check to see which comes first in list\n",
    "    else:\n",
    "        top_votes = c[0][1]\n",
    "        #print(top_votes)\n",
    "        poss = []\n",
    "        for t in c:\n",
    "            if t[1] == top_votes:\n",
    "                poss.append(t[0])\n",
    "        idx = dict()\n",
    "       # print(poss)\n",
    "        for p in poss:\n",
    "            idx[labels.index(p)] = p\n",
    "        #print(idx)\n",
    "        return labels[sorted(idx.keys())[0]]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 9:\n",
    "* 15 points\n",
    "\n",
    "Now it is time to put everything together. In questions 5 and 6 we defined functions to calculate distances. In question 7 we defined a function to sort and return n desired labels. Finally, in question 8 we counted the number of \"votes.\"\n",
    "\n",
    "The next question asks for a KNN modeling function.\n",
    "\n",
    "Define a function called custom_KNN that takes as inputs\n",
    "\n",
    "a single value from X_test (created above in our test_train_split)\n",
    "X_train\n",
    "the labels y_train\n",
    "n - the number of nearest neighbors to poll in making predictions.\n",
    "Your function should calculate the Euclidean distance between that X_test-point and every point in X_train and finds the labels from the n nearest neighbors ordered from the closest ones to the furthest ones.\n",
    "\n",
    "Your function should return a prediction according to the voting rules outlined in question 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "def custom_KNN( point, X_train, y_train, n):\n",
    "    \"\"\"\n",
    "    Predict the label for a single point, given training data and a specified\n",
    "    \"n\" number of neighbors.\n",
    "    \n",
    "    Positional Arguments:\n",
    "        point -- a pandas Series corresponding to an observation of a point with\n",
    "             unknown label.\n",
    "        x_train -- a pandas DataFrame corresponding to the measurements\n",
    "            of points in a dataset. Assume all values are numeric, and\n",
    "            observations are in the rows; features in the columns\n",
    "        y_train -- a pandas Series corresponding to the labels for the observations\n",
    "            in x_train\n",
    "    \n",
    "    Example:\n",
    "        point = pd.Series([1,2])\n",
    "        X_train = pd.DataFrame([[1,2],[3,4],[5,6]])\n",
    "        y_train = pd.Series([\"a\",\"a\",\"b\"])\n",
    "        n = 2\n",
    "        print(custom_KNN(point, X_train, y_train, n)) #--> 'a'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    dists = all_distances(point, X_train)\n",
    "    labs = labels_of_smallest(dists, y_train, n)\n",
    "    return label_voting(labs)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def custom_KNN(point, X_train, y_train, n):\n",
    "    \n",
    "    # Helper Function for vote counting\n",
    "    def countVotes(l):\n",
    "        c = Counter(l).most_common()\n",
    "        if len(c) == 1:\n",
    "            return c[0][0]\n",
    "        if c[0][1] > c[1][1]:\n",
    "            return c[0][0]\n",
    "        else:\n",
    "            top_votes = c[0][1]\n",
    "            #print(top_votes)\n",
    "            poss = []\n",
    "            for t in c:\n",
    "                if t[1] == top_votes:\n",
    "                    poss.append(t[0])\n",
    "            idx = dict()\n",
    "           # print(poss)\n",
    "            for p in poss:\n",
    "                idx[l.index(p)] = p\n",
    "            #print(idx)\n",
    "            return l[sorted(idx.keys())[0]]\n",
    "        \n",
    "    # Take difference \n",
    "    diff = point - X_train\n",
    "    \n",
    "    # Find distance\n",
    "    dists = np.apply_along_axis(np.linalg.norm, 1, diff )\n",
    "    \n",
    "    # Create df of distances; re-index to original data    \n",
    "    df = pd.DataFrame(dists)\n",
    "    df.index = X_train.index\n",
    "    \n",
    "    # Add labels, column names.\n",
    "    df = pd.concat([df, y_train], axis = 1)\n",
    "    \n",
    "    df.columns = [\"dist\",\"label\"]\n",
    "    \n",
    "    # Take top votes, and count\n",
    "    votes = list(df.sort_values(\"dist\").head(n)['label'])\n",
    "    return countVotes(votes)\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a functioning KNN classifier assigned to the function customKNN.\n",
    "\n",
    "Let's now see how good our classifier is using n = 5.\n",
    "\n",
    "The below cell may or may not complete running on Vocareum due to processing constraints.\n",
    "\n",
    "FOR FASTER COMPLETION, TRY COMMENTING OUT THE CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create new test train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=24)\n",
    "\n",
    "print(\"Total 'test' observations:\", len(X_test))\n",
    "print(\"Classifying every point in X_test would take too long - classify the first 100\")\n",
    "custom_preds = []\n",
    "for i, idx in enumerate(X_test.index[:100]):\n",
    "    if i % 100 == 0: print(i)\n",
    "    pred = custom_KNN(X_test.loc[idx,:], X_train, y_train, 5)\n",
    "    custom_preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN in sklearn\n",
    "While is useful to learn and see how predictions are made using K-Nearest Neighbors using our own function, the sklearn library has an implementation called KNeighborsClassifier that will run much faster than our home-built version.\n",
    "\n",
    "In the next question, we will ask you to implement KNN in sklearn.\n",
    "\n",
    "* Question 10:\n",
    "* 15 points\n",
    "\n",
    "Use the function KNeighborsClassifier to instantiate the classifier knn by setting the parameter n_neighbors = 5. Next, use the function fit() to fit the X and y training sets to the classifier.\n",
    "\n",
    "Finally, create a prediction for the first 100 test obervation. Define this prediction to be skpreds This can be accomplished by using the function predict on the classifier knn by setting the argument X_test[:100].\n",
    "\n",
    "Feel free to refer to the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "# Import\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "knn = None\n",
    "skpreds = None\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# Fit model with training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Create predictions for first 100 test observations\n",
    "skpreds = knn.predict(X_test[:100])\n",
    "\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, we compare the performance of our home build knn classifier and the one that comes with sklearn. If we have made no mistakes, the difference should equal zero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sklearn prediction performance\")\n",
    "print(classification_report(y_test[:100], skpreds))\n",
    "\n",
    "\n",
    "### The below lines of code will compare the performance of your home-built classification with\n",
    "### The sklearn predictions -- if all the cells above were run sucessfully, you should see identical scores\n",
    "\n",
    "print(\"\\nHome-Built prediction performance\")\n",
    "print(classification_report(y_test[:100], custom_preds))\n",
    "\n",
    "\n",
    "### The below lines of code will explicitly compare predictions:\n",
    "### \"differences\" should == 0!\n",
    "\n",
    "### NB: Commenting/uncommenting multiple lines in Jupyter can be accomplished with:\n",
    "### <ctrl-/> on windows and <cmd-/> on mac\n",
    "differences = 0\n",
    "for cust, sk in zip(custom_preds, skpreds):\n",
    "    if cust != sk:\n",
    "        differences +=1\n",
    "print(\"Total Differences:\", differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice with sklearn:\n",
    "In the questions below, we will ask you to create a new test/train split, and fit a new KNN model using sklearn.\n",
    "\n",
    "All of the basic steps for KNN have already been performed above. Feel free to reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Data is consistent\n",
    "\n",
    "# read feature names\n",
    "feats = pd.read_csv(FEATURE_NAMES, sep='\\n', header=None)\n",
    "\n",
    "# read in training data\n",
    "har_train = pd.read_csv(TRAIN_DATA, sep='\\s+', header=None)\n",
    "\n",
    "# read in training labels, and clean them.\n",
    "har_train_labels = pd.read_csv(TRAIN_LABELS, sep='\\n', header=None)\n",
    "clean_features = [feat[0].split(' ')[1] for feat in feats.values]\n",
    "har_train.columns = clean_features\n",
    "\n",
    "har_train_labels = pd.read_csv(TRAIN_LABELS, sep='\\n', header=None)\n",
    "har_train_labels.columns = ['label']\n",
    "y = har_train_labels.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 11:\n",
    "* 5 points\n",
    "\n",
    "Suppose you want to make a new test-train-split on our data such that the labels should be found in y and observations should be found in har_train. When splitting, we want to assign the output from the split to X_train2, X_test2, y_train2, and y_test2. We want to set test_size equal to .4 and the random_state to 1738. Which of the following commands would accomplish this task?\n",
    "\n",
    "'a') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, test_size = .4, random_state = 1738)\n",
    "'b') X_train2, X_test2, y_train2, y_test2 = train_test_split(y, har_train, train_size = .4, random_state = 1738)\n",
    "'c') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, .4, 1738)\n",
    "'d') X_train2, X_test2, y_train2, y_test2 = train_test_split(har_train, y, t_size = .4, rs = 1738)\n",
    "Assign the letter associated with you choice as string to ans11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "ans11 = None\n",
    "### BEGIN SOLUTION\n",
    "ans11 = 'a'\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a model using sklearn is just as easy as those last two steps! As long as your data is in the right format, once you make your train/test split, the syntax for fitting pretty much any of the models in sklearn is about the same.\n",
    "\n",
    "Interpret the results\n",
    "To interpret the results we will be looking at the trade-off between bias and variance as we change our n_neighbors. In many cases, false negatives are more costly than false positives. As such we will be looking primarily at the change in recall as we build a number of different models.\n",
    "\n",
    "Note: The code below takes some time to run and it may time out on Vocareum. For this reason, we have commented out the code for you and just put the image produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from sklearn.metrics import recall_score\n",
    "\n",
    "### Calculating Recal scores for multiple \"n-neighbors\"\n",
    "#recall_scores = {}\n",
    "#for n in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,20,25,50,75,100]:\n",
    " #    knn = KNeighborsClassifier(n_neighbors=n)\n",
    " #   knn.fit(X_train, y_train)\n",
    " #    recall_scores[n] = recall_score(y_test, knn.predict(X_test), average = None)\n",
    "    \n",
    "### Put recall scores into DataFrame\n",
    "#scores_df = pd.DataFrame(recall_scores).T\n",
    "#scores_df.columns = [str(i) for i in range(1,7)]\n",
    "#scores_df.index = scores_df.index.astype(str)\n",
    "\n",
    "### Create plot of recall scores\n",
    "#plt.figure(figsize = (10,10))\n",
    "#for col in scores_df:\n",
    " #    if col != 'n_neighbors':\n",
    " #      plt.plot(scores_df[col], label = col)\n",
    "    \n",
    "#plt.ylabel(\" Recall Score\", fontsize = 12)\n",
    "#plt.xlabel(\"n_neighbors (NB: not an interval scale)\", fontsize = 12)\n",
    "#plt.legend(title = \"activity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the recall scores above, as n_neighbors trends towards 100 we see in increase in bias. Furthermore, it looks like the better KNN models have less than 15 n_neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
