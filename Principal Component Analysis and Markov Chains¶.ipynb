{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis and Markov Chains\n",
    "Author: Jessica Cervi\n",
    "\n",
    "Expected time = 2.5 hours\n",
    "\n",
    "Total points = 80 points\n",
    "\n",
    "Assignment Overview\n",
    "In this assignment, we will work on principal component analysis. Principal component analysis (PCA) is a technique for reducing the dimensionality of datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance.\n",
    "\n",
    "In the second part of the assignment we will explain some basic concepts and familiarize with the theory about Markov chains. We will introduce you to one of the Python libraries, quantecon that come with the capability of simulating Markov chains processes. This second part of the assignment is intended to be more theory heavy, most of the coding about Markov chain will be asked for you to do in the next assignment.\n",
    "\n",
    "This assignment is designed to build your familiarity and comfort in coding in Python. It will also help you review the key topics from each module. As you progress through the assignment, answers to the questions will get increasingly complex. You must adopt a data scientist's mindset when completing this assignment. Remember to run your code from each cell before submitting your assignment. Running your code beforehand will notify you of errors and giving you a chance to fix your errors before submitting it. You should view your Vocareum submission as if you are delivering a final project to your manager or client.\n",
    "\n",
    "Vocareum Tips\n",
    "\n",
    "Do not add arguments or options to functions unless asked specifically. This will cause an error in Vocareum.\n",
    "Do not use a library unless you are explicitly asked in the question.\n",
    "You can download the Grading Report after submitting the assignment. It will include the feedback and hints on incorrect questions.\n",
    "Learning Objectives\n",
    "Understand the implementation of principal components analysis using sklearn\n",
    "Understand the importance of preparing and scaling the data\n",
    "Understand and interpret the results\n",
    "Understand the mathematical foundations of Markov chains\n",
    "Implement Markov chains using quantecon\n",
    "Understand and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis and Markov Chains\n",
    "Importing the dataset and exploratory data analysis\n",
    "In the first part of this assignment, we will be using a data set that includes the information about 721 Pokemon, including their number, name, first and second type, and basic stats: HP, Attack, Defense, Special Attack, Special Defense, and Speed.data modeled from Amazon reviews. A description of the Pokemon data and instructions to download the database can be found here. We begin by importing the libraries we will use in this assignment.\n",
    "\n",
    "We begin the assignment by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the read_csv function to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Pokemon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:\n",
    "* 5 points\n",
    "\n",
    "*Rename the column # to id. Additionally, create a list, cols, of columns we will consider later. In this list include the columns 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "cols = None\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "columns = df.columns.tolist()\n",
    "columns[0] = 'id'\n",
    "df.columns = columns\n",
    "\n",
    "# Selecting columns to consider\n",
    "cols = ['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing the PCA, we need to scale the data so that the distribution of HP, Attack, and the other columns we are interested in are centered around 0 with a standard deviation of 1.\n",
    "\n",
    "Note that we do not consider the column Total because that is the sum of the following columns.\n",
    "\n",
    "\n",
    "\n",
    "* Question 2:\n",
    "* 10 points\n",
    "\n",
    "We can scale our data by using the function StandardScaler from sklearn imported above. You can find the documentation for this function here.\n",
    "\n",
    "Instantiate a scaler, scaler, by using the function StandardScaler. Make sure that you fit the scaler only to the columns we are interested on (the ones listed in cols).\n",
    "\n",
    "Next, apply the method transform to your scaler by passing df[cols] as an argument. Redefine this new object to be df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "df_scaled = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "scaler = StandardScaler().fit(df[cols])\n",
    "df_scaled = scaler.transform(df[cols])\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done things correctly, the mean of your scaled data should be around 0 and the stardard deviation should be close to one.\n",
    "\n",
    "Let's verify this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_scaled[:,0].mean())  # zero (or very close)\n",
    "print(df_scaled[:,0].std()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "In the code cell below, we have initialized the pca class using the sklearn function PCA. Note that we consider 80% of our data.\n",
    "\n",
    "Next we fit our class the df_scaled and we apply dimensionality reduction to obtain a new dataframe with our PCA scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8)  \n",
    "#Fit the model\n",
    "pca.fit(df_scaled)\n",
    "#Apply dimesionality reduction and obtain a new dataframe\n",
    "pcscores = pd.DataFrame(pca.transform(df_scaled))\n",
    "#Renaming columns\n",
    "pcscores.columns = ['PC'+str(i+1) for i in range(len(pcscores.columns))]\n",
    "loadings = pd.DataFrame(pca.components_, columns=cols)\n",
    "loadings.index = ['PC'+str(i+1) for i in range(len(pcscores.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcscores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the PCA does is construct principal components that explain most of the variance or scatter of the original dataset. Each component is a linear combination of all the variables and is perpendicular to every other component. Each variable in each component is multiplied by a set of factors, the loading factors, which transforms the original data into this new component space. These loading factors are constrained so that the square of the sum is equal to 1, hence they can serve as weights to see which parameters are most important for a particular principal component.\n",
    "\n",
    "Let's look at that in more detail with some figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sqr = loadings**2\n",
    "\n",
    "\n",
    "ax = sns.heatmap(load_sqr.transpose(), linewidths=0.5, cmap=\"BuGn\", annot=True)\n",
    "#Setting the ticks\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=0, fontsize=8)\n",
    "ax.set_yticklabels(ax.yaxis.get_majorticklabels(), rotation=0, fontsize=8)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darkest shades in the plot above indicate which parameters are the most important. For example, the loading factors for PC4 show that HP is the most dominant parameter. That is, Pokemon with high HP will have high absolute values of PC4.\n",
    "\n",
    "Let's look at the actual values of the loading factors now:\n",
    "\n",
    "Below, we have created heatmap, like the one provided above, using the data from the loadings dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(loadings.transpose(),  annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see some more trends. For example, a Pokemon with high Defense or low Speed will have a positive value of PC2. On the other hand, things like Attack or Sp. Defense will control what value a Pokemon will have for PC3.\n",
    "\n",
    "Yet another way to look at this is to examine the data with a biplot, which is a scatter plot with vectors indicating what direction a datapoint will take in the PCA given its underlying parameters. For fun, the Pokemon are color-coded by Type to see if there is any obvious trends.\n",
    "\n",
    "In the code cell below, we define a function to create the biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels based on Type 1\n",
    "labels = set(df['Type 1'])\n",
    "df['type'] = df['Type 1']\n",
    "lab_dict = dict()\n",
    "for i, elem in enumerate(labels):\n",
    "    lab_dict[elem] = i\n",
    "df = df.replace({'type' : lab_dict})\n",
    "\n",
    "pc_types = pcscores.copy()\n",
    "pc_types['Type'] = df['Type 1']\n",
    "\n",
    "# Biplots\n",
    "def make_plot(pcscores, loadings, xval=0, yval=1, max_arrow=0.2, alpha=0.4):\n",
    "    n = loadings.shape[1]\n",
    "    scalex = 1.0 / (pcscores.iloc[:, xval].max() - pcscores.iloc[:, xval].min())  # Rescaling to be from -1 to +1\n",
    "    scaley = 1.0 / (pcscores.iloc[:, yval].max() - pcscores.iloc[:, yval].min())\n",
    "\n",
    "    pcscores.iloc[:, xval] = pcscores.iloc[:, xval] * scalex\n",
    "    pcscores.iloc[:, yval] = pcscores.iloc[:, yval] * scaley\n",
    "\n",
    "    g = sns.lmplot(x='PC{}'.format(xval + 1), y='PC{}'.format(yval + 1), hue='Type', data=pcscores,\n",
    "                   fit_reg=False, size=6, palette='muted')\n",
    "\n",
    "    for i in range(n):\n",
    "        # Only plot the longer ones\n",
    "        length = sqrt(loadings.iloc[xval, i] ** 2 + loadings.iloc[yval, i] ** 2)\n",
    "        if length < max_arrow:\n",
    "            continue\n",
    "\n",
    "        plt.arrow(0, 0, loadings.iloc[xval, i], loadings.iloc[yval, i], color='k', alpha=0.9)\n",
    "        plt.text(loadings.iloc[xval, i] * 1.15, loadings.iloc[yval, i] * 1.15,\n",
    "                 loadings.columns.tolist()[i], color='k', ha='center', va='center')\n",
    "\n",
    "    g.set(ylim=(-1, 1))\n",
    "    g.set(xlim=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 3:\n",
    "* 10 points\n",
    "\n",
    "Using the function make_plot above, create a biplot. Set the argument pcscores equal to pc_types, loadings equal to loadings, xval equal to 2, yval equal to 3 and max_arrow equal to 0.3.\n",
    "\n",
    "Save your image as plot4.png inside the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "make_plot(pc_types, loadings, 2, 3, max_arrow=0.3)\n",
    "plt.savefig(\"results/plot4.png\")\n",
    "plt.close()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see that Pokemon are primarily centrally distributed; that is, their stats are fairly balanced. There don't appear to be any obvious trends with type. There are some outliers, for example, 2 normal type Pokemon with high values of PC4. If you recall, PC4's loading factors indicated that HP was the dominant parameter.\n",
    "\n",
    "Back to top\n",
    "\n",
    "* Question 4:\n",
    "* 10 points\n",
    "\n",
    "Use the sort_values function to sort the pc_types dataframe by PC4 in descending order. Extract only the first two rows and assign those to the dataframe best_pc4.\n",
    "\n",
    "Use best_pc4 to filter the rows of df using the function loc. Assign the result to df_best_pc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "best_pc4 =  None\n",
    "df_best_pc4 = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "best_pc4 = pc_types.sort_values(by='PC4', ascending=False)[:2]\n",
    "df_best_pc4 = df.loc[best_pc4.index]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also decide to check which Pokemon has the highest 'HP':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='HP', ascending=False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 5:\n",
    "* 5 points\n",
    "\n",
    "Use the sort_values function to sort the pc_types dataframe by PC2 in descending order. Extract only the first row and assign those to the dataframe best_pc2.\n",
    "\n",
    "Next, use the sort_values function to sort the pc_types dataframe by PC3 in descending order. Extract only the first row and assign those to the dataframe best_pc3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "best_pc2 =  None\n",
    "best_pc3 = None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "best_pc2 = pc_types.sort_values(by='PC2', ascending=False)[:1]\n",
    "best_pc3 = pc_types.sort_values(by='PC3', ascending=False)[:1]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell, we print best_pc2 and best_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_pc2)\n",
    "print(best_pc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that this is really the same Pokemon! Let's see which one from our original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[230]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 6:\n",
    "* 10 points\n",
    "\n",
    "Use the function make_plot above, create a biplot. Set the argument pcscores equal to pc_types, loadings equal to loadings, xval equal to 1, yval equal to 2 and max_arrow equal to 0.3.\n",
    "\n",
    "Save your image as plot7.png inside the results folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "make_plot(pc_types, loadings, 1, 2, max_arrow=0.3)\n",
    "plt.savefig(\"results/plot7.png\")\n",
    "plt.close()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this Pokemon is located towards the top right (high PC2 and PC3 values).\n",
    "\n",
    "Of course, we could go ahead and experiment with many other features. As a conclusion, one powerful thing of the PCA is the dimensionality reduction aspect of it. We went from having 6 variables to consider to only having 4. From this, it may be possible to examine fits or create classification models.\n",
    "\n",
    "In the second part of this assignment, we will work with Markov chains.\n",
    "\n",
    "Markov chains are one of the most useful classes of stochastic processes, being\n",
    "\n",
    "simple, flexible and supported by many elegant theoretical results\n",
    "valuable for building intuition about random dynamic models\n",
    "central to quantitative modeling in their own right\n",
    "In this assignemnt, we review some of the theory of Markov chains and implement a few simple exercises from scratch.\n",
    "\n",
    "Before starting, we will review some fundamentals concepts:\n",
    "\n",
    "Stochastic Matrices\n",
    "A stochastic matrix (or Markov matrix) is an  n×n  square matrix  P  such that each element of  P  is nonnegative, and each row of  P  sums to one. Each row of  P  can be regarded as a probability mass function over  n  possible outcomes.\n",
    "\n",
    "Of course, if  P  is a stochastic matrix, then so is the  k -th power  Pk .\n",
    "\n",
    "Markov Chains\n",
    "There is a close connection between stochastic matrices and Markov chains.\n",
    "\n",
    "To begin, let  S  be a finite set with  n  elements  {x1,…,xn} . The set  S  is called the state space and  x1,…,xn  are the state values.\n",
    "\n",
    "A Markov chain  {Xt}  on  S  is a sequence of random variables on  S  that have the Markov property.\n",
    "\n",
    "This means that, for any date  t  and any state  y∈S \n",
    "P{Xt+1=y|Xt}=P{Xt+1=y|Xt,Xt−1,…}\n",
    " \n",
    "In other words, knowing the current state is enough to know probabilities for future states.\n",
    "\n",
    "We begin by importing the necessary libraries for this part of the assignment.\n",
    "\n",
    "NOTE: There are many Python libraries that offer an implementation of Markov Chains, however, most of them do not allow for a random seed to be set. For this reason, results are not reproducible. For this part of the assignment we will mostly ask you about setting up simple problems. The Markov chains functions call from quantecon will be only demonstrated to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantecon as qe\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 7:\n",
    "* 5 points\n",
    "\n",
    "Let's begin with a very simple example. Define a tuple, prob with probabilities in [0,1] with entries 0.3 and 0.7. Next, use the NumPy function cumsum to convert your probabilities into a cumulative distribution function. Assign this to the variable cdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "prob = None\n",
    "cdf = None\n",
    "### BEGIN SOLUTION\n",
    "prob = (0.3, 0.7)        \n",
    "cdf = np.cumsum(prob)  \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate, say, 10 independent draws from cdf we can use the function random.draw from quantecon, like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qe.random.draw(cdf, 10)   # generate 10 independent draws from our cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 8:\n",
    "* 15 points\n",
    "\n",
    "Next, let's try to define our own function to simulate a simple Markov chain process.\n",
    "\n",
    "Define a function mc_sample_path that takes three arguments:\n",
    "\n",
    "A stochastic matrix P\n",
    "A cumulative distribution function, cdf. Set this argument equal to None by default\n",
    "An integer, sample_size. Set this argument equal to 1000 by default\n",
    "Your function should execute the following steps:\n",
    "\n",
    "Convert P to a Numpy array. Use the function asarray\n",
    "Create a new Numpy array X with shape equal to sample_size and type int, without initializing entries. For this, use the function empty.\n",
    "Convert each row of P into a cumulative distribution function. You can achieve this using a list comprehension.\n",
    "If cdf is not None, use the function draw from quantecon on cdf using the NumPy function cumsum to generate an initial state X_0. If cdf is None set the initial state X_0 = 0.\n",
    "Set the first entry of X equal to X_0.\n",
    "Use a for loop to simulate your sample_size -1 draws. HINT: the following state can be computed via X[t+1] = qe.random.draw(P_dist[X[t]])\n",
    "Your function should return one minus the mean of the array X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def mc_sample_path():\n",
    "    return\n",
    "### BEGIN SOLUTION\n",
    "def mc_sample_path(P, cdf=None, sample_size=1000):\n",
    "\n",
    "    # set up\n",
    "    P = np.asarray(P)\n",
    "    X = np.empty(sample_size, dtype=int)\n",
    "\n",
    "    # Convert each row of P into a cdf\n",
    "    n = len(P) \n",
    "    P_dist = [np.cumsum(P[i, :]) for i in range(n)]\n",
    "\n",
    "    # draw initial state, defaulting to 0\n",
    "    if cdf is not None:\n",
    "        X_0 = qe.random.draw(np.cumsum(cdf))\n",
    "    else:\n",
    "        X_0 = 0\n",
    "\n",
    "    # simulate\n",
    "    X[0] = X_0\n",
    "    for t in range(sample_size - 1):\n",
    "        X[t+1] = qe.random.draw(P_dist[X[t]])\n",
    "\n",
    "    return 1- np.mean(X)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how it works using the small matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [[0.4, 0.6],\n",
    "     [0.2, 0.8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we’ll see later, for a long series drawn from P, the fraction of the sample that takes value 1 will be about 0.75.\n",
    "\n",
    "Moreover, this is true, regardless of the initial distribution from with X_0 is drawn.\n",
    "\n",
    "The following code illustrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mc_sample_path(P, sample_size=100000) \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using QuantEcon’s Routines\n",
    "As discussed above, QuantEcon has routines for handling Markov chains, including simulation.\n",
    "\n",
    "Here’s an illustration using the same P as the preceding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantecon import MarkovChain\n",
    "\n",
    "mc = qe.MarkovChain(P)\n",
    "X = mc.simulate(ts_length=100000)\n",
    "np.mean(X == 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Of course, the quantecon routine is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mc_sample_path(P, sample_size=5000000) # Our version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irreducibility\n",
    "Irreducibility is a central concept of modern Markov chain theory.\n",
    "\n",
    "Let P be a fixed stochastic matrix. Two states x and y are said to communicate with each other if there exist positive integers j and k such that\n",
    "Pj(x,y)>0\n",
    " \n",
    "and\n",
    "Pk(y,x)>0\n",
    " \n",
    "In view of our discussion above, this means precisely that state x can be reached eventually from state y, and state y can be reached eventually from state x The stochastic matrix P is called irreducible if all states communicate.\n",
    "\n",
    "For example, consider the following transition probabilities for wealth of set of households\n",
    "\n",
    "\n",
    "\n",
    "We can translate this into a stochastic matrix, putting zeros where there’s no edge between nodes. It’s clear from the graph that this stochastic matrix is irreducible: we can reach any state from any other state eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [[0.9, 0.1, 0.0],\n",
    "     [0.4, 0.4, 0.2],\n",
    "     [0.1, 0.1, 0.8]]\n",
    "\n",
    "mc = qe.MarkovChain(P, ('poor', 'middle', 'rich'))\n",
    "mc.is_irreducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question 9:\n",
    "* 10 points\n",
    "\n",
    "Imagine now that transition probailities for wealth of set of households get updated like the following.\n",
    "\n",
    "\n",
    "\n",
    "Define a new stochastic matrix for this problem like in the example above, and assign it to the variable P. Will the stochastic matrix still be reducible? Assign the boolean value to ans10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED \n",
    "\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "ans10 = None\n",
    "### BEGIN SOLUTION\n",
    "P = [[1.0, 0.0, 0.0],\n",
    "     [0.1, 0.8, 0.1],\n",
    "     [0.0, 0.2, 0.8]]\n",
    "\n",
    "mc = qe.MarkovChain(P, ('poor', 'middle', 'rich'))\n",
    "ans10 = mc.is_irreducible\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
